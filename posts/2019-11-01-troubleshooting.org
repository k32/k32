---
title: How to fix anything without a clue
tags: ramblings, software engineering, troubleshooting
---

In this post I want to share some techniques that I developed over the
years of troubleshooting large-scale mission-critical systems. This
list will be updated when I come up with more useful generalizations.

"In the ideal world this is absolutely impossible!", some of the
readers may exclaim in terror and disgust, "This is a bad
practice!". I will address this in a chapter called "Have zero trust
in humanity". But even if your service has no bugs whatsoever, has
rock-solid deployment pipeline, and /cannot possibly go wrong/, it
still lives in the shadow of a huge fan, that other, less reliable,
code flies by at near-miss distance.

You can also approach troubleshooting from a more positive
perspective. Imagine you use an opensource product that lacks a
feature you want. You want to implement this feature, but where to
begin?  Techniques described below will help to navigate through
unfamiliar source code quickly.

* Essential dos and don'ts

* Don'ts

** Don't debug in production

First and foremost: don't use debug tools in the production
environment. Debuggers or excessive logging put a lot of stress on the
system, so much so that it will likely explode under high
traffic. There are introspection tools that are /more or less/ safe to
use in a live environment, but one should be careful with those
too. I'll discuss some of these tools below.

Logging in to a live system to run a debug command is reckless enough
already, but preparing an instruction for an Ops team how to do it,
should be absolutely out of question! Suppose you verified the
instruction on a test node, so you know that it finishes in a couple
seconds. Then you login to production, and the same command hangs for
more than 10 seconds. You realize that you screwed up somewhere, and
may have a chance to abort operation before SSH session dies. The
other person won't react as quickly.

** Don't try to understand how things work

...and focus on why they don't work.

Second big mistake: don't try to understand the entire code
base. Typical enterprise system consists of hundreds KLOC not counting
dependencies, it was written by dozens of engineers over the years,
and it interacts with a few other systems. It's just not a fair match
against a single person (you) working under time pressure.  If you try
to find out why error happens by following the code ("a function that
crashes is called from here, where data comes from there"), you'll get
lost very quickly. You can try this approach, and it /may/ work with
the most obvious bugs, but keep in mind that its complexity grows
exponentially. If you find yourself looking through more than, say, 3
source code files at the same time, consider switching to more clever
techniques described below.

Understanding how the system works /will/ be required later, when you
work on a "proper" solution, but during peak firefighting stage this
knowledge is unobtainable, so you need to be prepared to perform on
stage without a clue.

* Dos

** Estimate urgency

Impact of software issues varies from "batch job is delayed, and
quality of service is slowly declining" to "company is losing =$$$$/s=
/right now/". Depending on the situation, more aggressive
troubleshooting techniques become appropriate.

** Have you tried turning it off and on?

Don't underestimate the importance of "fixing stuff by
power-cycling". It's efficient in, like, 90% cases. If you're entering
on-call rotation, learn how to restart your services, and what is the
impact of doing so.

* Advanced techniques

* Differential troubleshooting

This technique is absolutely essential. It is simple, but exceedingly
efficient. In short, it's a way of reasoning based around questions:

 - Anomaly was reported at 10am, what log messages appeared (or
   disappeared) around this time?
 - It worked before release R, and it doesn't work now. What was in
   release R?
 - It works with data A, but not data B. What is the difference
   between A and B?

In other words, how does an anomaly /change/ the behavior of the
system.

The more specific your questions become, the smaller is your search
space. Your goal is to narrow down difference to the smallest possible

** Passive

** Active

* Bisection

* Entropy minimization

* Tools

** Metrics

* Have zero trust in humanity

If you find yourself dismissing some idea because of "well, this would
be too obvious" or "no, this is absolutely impossible" reasons,
chances are that actually you are onto something. After seeing bugs
that no person should see, I conclude with all confidence, that our
industry is absolutely cursed. No one has any idea how to do
things. Putting your trust in our ability to do anything sane is like
lending your wallet to a bunch of thugs.

This raises a question: if nothing can be trusted, how to avoid
depth-first'ing the entire OS and hardware stack? The answer lies in
differential troubleshooting technique described above. You can
suspect a bug in Linux. But if you do so, it means the bug should
manifest not only in your business application, but in all processes
running on the same host. If you don't observe anomalies in the other
processes, OS bug is less likely than application bug. Bisection is
also useful: if you suspect a Linux kernel bug, run strace to find if
data that goes into the kernel is valid (most likely you will find
that it's not).

If you know OS and networking level well enough, and you practice
differential troubleshooting routine, your brain can generate and
dismiss potential failure scenarios in split second, so suspecting
services and libraries outside of your own is not as time-consuming as
one may think.

* Don't work alone

When nothing works, try to get some company. Worst thing that can
happen is when you panic and stop trying new ideas. This happens even
to the best of us. Brainstorming helps a lot, but just having a
friendly chat, while something is burning, helps people staying
productive.

* Nightmare difficulty: "Zero-knowledge troubleshooting"

Any bug is a cakewalk, when it concerns a system that you actively
work on. But I bet your company has a quite a few Great Old Ones
sleeping underneath Jira. And things get much more picante when
you first hear about such system from a bug report.

I know, I know, situation like this could /never/ happen in the ideal
world. But if you're reading this post, then your plane of existence
intersects with mine. Be wary: in hellish place that I inhibit, people
retire, change teams, and there are 10x rockstar ninja wizards who
develop something just to put it on their resum√© and hop onto a next
job. If you receive a trouble report related to one of these systems,
and have no idea what it does and where it lives, don't worry too
much. There is a person who knows that: one who submitted the bug
report. Interrogate them until you find some entry points towards the
system. Learn hostnames, keywords, what is the expected behavior, what
behavior is unexpected, and so on. Then use source code index and all the
techniques described above.

P.S. If you find yourself solving this kind of problem often, look
around and check if you're the last person left in the
office. Consider tactical retreat towards the job market.

* Philosophical digression: search space

In the last chapter I'd like to ramble a little about .

Search space is a Cartesian product of

 - Set of valid values of instruction pointer in the application's
   executable code
 - Input data
 - Application's transient state
 - Application's persistent state
 - State of hardware and OS
 - State of the surrounding network
 - State of external services the application depends on
 - ...

And it's large. Some of places in this set are unreachable (functional
programs, memory protection, blah-blah). Some places are explored
during testing. Some subsets correspond to

* Epilogue

This knowledge will ruin your life. If you master these techniques,
don't let anyone know.

The best way to apply your troubleshooting skills is by developing new
systems, rather than keeping legacy code on life support. Most
appropriate time for bug hunting is /before/ the software goes
live. Good troubleshooters make the best programmers, because they've
learned from others' mistakes. They tend to design systems that are
more resilient to failure, and that are easier to troubleshoot. They
intuitively see scenarios that should be covered by tests. They
learned negative, pessimistic way of thinking, that is essential for
any platform-layer designer.

Note that I don't encourage you to program defensively, instead:

 - Physically separate systems that should have AAA reliability from
   the systems those code should be able to mutate quickly following
   business processes
 - Design systems that have redundancy
 - Design systems that fail safe
 - Employ good DevOps practices, such as blue-green deployments,
   canary deployments, gradual roll-out of features, A/B tests, chaos
   monkeys and what not
 - Keep it simple, stupid. When you write a line of code, imagine that
   someone has to troubleshoot it at 4 am, and they are good at
   this. Which means they /will/ find your hideout
