#+TITLE: How to troubleshoot anything without a clue

In this post I want to share some techniques that I developed over the
years of troubleshooting large systems. This list will be updated when
I come up with more useful generalizations.

"In the ideal world this is absolutely impossible!", some of the
readers may exclaim in terror and disgust, "This is a bad
practice!". I will address this in a chapter called "Have zero trust
in humanity".

Although in most cases troubleshooting is a consequence of human
imperfection, you can approach it from a more positive
perspective. Imagine you use an opensource product that lacks a
feature you want. You want to implement this feature, but where to
begin? Techniques described below will help to navigate through
unfamiliar code bases quickly.

* Essential dos and don'ts

** Don't debug in production

First and foremost: don't use debug tools in the production
environment. Debuggers or excessive logging put a lot of stress on the
system, so much so it will likely explode under high traffic. There
are introspection tools that are /more or less/ safe to use in a live
environment, but one should be careful with those too. I'll discuss
some of these tools below.

Logging in to a live system to run a debug command is reckless enough
already, but preparing an instruction for an ops team how to do it,
should be absolutely out of question! Suppose you verified the
instruction on a test node, so you know that it finishes in a couple
seconds. Then you login to production, and the same command hangs for
more than 10 seconds. You realize that you screwed up somewhere, and
may have a chance to abort operation before SSH session dies. The
other person won't react as quickly.

** Don't try to understand how things work

...and focus on why they don't work.

Second big mistake: don't try to understand the entire code
base. Typical enterprise system consists of hundreds KLOC not counting
dependencies and the underlying stack, it was written by dozens of
engineers over the years, and it interacts with a few other
systems. It's just not a fair match against a single person working
under time pressure.  If you try to find out why error happens by
following the code ("a function that crashes is called from here,
where data comes from there"), you'll get lost very quickly. You can
try this approach, and it /may/ work with the most obvious bugs, but
keep in mind that its complexity grows exponentially. If you find
yourself looking through more than, say, 3 source code files at the
same time, consider switching to more clever techniques described
below.

Understanding how the system works /will/ be required later, when you
work on a "proper" solution, but during peak firefighting stage this
knowledge is unobtainable, so you need to be prepared to perform on
stage without a clue.

** Estimate urgency

Impact of software issues varies from "batch job is delayed, and
quality of service is slowly declining" to "company is losing =$$$$/s=
/right now/". Depending on the situation, more aggressive
troubleshooting techniques become appropriate.

** Have they tried turning it off and on?

Don't underestimate the importance of "fixing bugs by
power-cycling". The ops team needs the runbooks describing how (and
when) to restart the misbehaving node.

* Advanced techniques

* Differential troubleshooting

This technique is absolutely essential. It is simple, but exceedingly
efficient. In short, it's a way of reasoning based around questions:

 - Anomaly was reported at 10am, what log messages appeared (or
   disappeared) around this time?
 - It worked before release R, and it doesn't work now. What was in
   the release R?
 - It works with data A, but not data B. What is the difference
   between A and B?

In other words, you need to find how does an anomaly /change/ the
behavior of the system. The more specific your questions become, the
smaller is your search space. Your goal is to narrow down the
difference between working and broken state of the system to the
smallest possible set.

Importance of this method is based on an assumption that any
production system is /mostly/ correct in a sense that it exhibits
somewhat sane behavior in 90% of cases. If it was just broken through
and thorough, it wouldn't have survived, and nobody wouldn't have
missed it. Therefore trying to find an error simply by following the
common code path is fruitless, because this code path is /most likely/
correct.

** Passive

** Active

* Bisection

* Entropy minimization

* Tools

** Metrics

* Have zero trust in humanity

If you find yourself dismissing some idea because of "well, this would
be too obvious" or "no, this is absolutely impossible" reasons,
chances are that actually you are onto something. After seeing bugs
that no person should see, I conclude with all confidence, that our
industry is absolutely cursed. No one has any idea how to do
things. Putting your trust in our ability to do anything sane is like
lending your wallet to a bunch of thugs.

This raises a question: if nothing can be trusted, how to avoid
depth-first'ing the entire OS and hardware stack? The answer lies in
differential troubleshooting technique described above. You can
suspect a bug in Linux. But if you do so, it means the bug should
manifest not only in your business application, but in all processes
running on the same host. If you don't observe anomalies in the other
processes, OS bug is less likely than application bug. Bisection is
also useful: if you suspect a Linux kernel bug, run strace to find if
data that goes into the kernel is valid (most likely you will find
that it's not).

If you know OS and networking level well enough, and you practice
differential troubleshooting routine, your brain can generate and
dismiss potential failure scenarios in split second, so suspecting
services and libraries outside of your own is not as time-consuming as
one may think.

* Don't work alone

When nothing works, try to get some company. Worst thing that can
happen is when you panic and stop trying new ideas. This happens even
to the best of us. Brainstorming helps a lot, but just having a
friendly chat, while something is burning, helps people staying
productive.

* Nightmare difficulty: "Zero-knowledge troubleshooting"

Any bug is a cakewalk, when it concerns a system that you actively
work on. But I bet your company has a quite a few Great Old Ones
sleeping underneath Jira. And things get much more picante when
you first hear about such system from a bug report.

I know, I know, situation like this could /never/ happen in the ideal
world. But if you're reading this post, then your plane of existence
intersects with mine. Be wary: in hellish place that I inhibit, people
retire, change teams, and there are 10x rockstar ninja wizards who
develop something just to put it on their resum√© and hop onto a next
job. If you receive a trouble report related to one of these systems,
and have no idea what it does and where it lives, don't worry too
much. There is a person who knows that: one who submitted the bug
report. Interrogate them until you find some entry points towards the
system. Learn hostnames, keywords, what is the expected behavior, what
behavior is unexpected, and so on. Then use source code index and all the
techniques described above.

P.S. If you find yourself solving this kind of problem often, look
around and check if you're the last person left in the
office. Consider tactical retreat towards the job market.

* Philosophical digression: search space

In the last chapter I'd like to ramble a little about .

Search space is a Cartesian product of

 - Set of valid values of instruction pointer in the application's
   executable code
 - Input data
 - Application's transient state
 - Application's persistent state
 - State of hardware and OS
 - State of the surrounding network
 - State of external services the application depends on
 - ...

And it's large. Some of places in this set are unreachable (functional
programs, memory protection, blah-blah). Some places are explored
during testing. Some subsets correspond to

* Epilogue

This knowledge will ruin your life. If you master these techniques,
don't let anyone know.

The best way to apply your troubleshooting skills is by developing new
systems, rather than keeping legacy code on life support. Most
appropriate time for bug hunting is /before/ the software goes
live. Good troubleshooters make the best programmers, because they
learned from others' mistakes. They tend to design systems that are
more resilient to failure, and that are easier to troubleshoot. They
intuitively see scenarios that should be covered by tests. They
learned negative, pessimistic way of thinking, that is essential for
any platform-layer designer.

Note that I don't encourage you to program defensively, instead:

 - Separate systems that should have AAA reliability from the systems
   those code should be able to mutate quickly following business
   processes
 - Design systems that have redundancy
 - Design systems that fail safe
 - Employ good deployment practices
 - Keep it simple, stupid. When you write a line of code, imagine that
   someone has to troubleshoot it at 4 am, and they are good at
   this. Which means they /will/ find your hideout
